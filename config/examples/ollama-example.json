{
  "//": "OpenJRAgent Configuration Example - Ollama Local Models",
  "//": "This is a complete example configuration for using Ollama local models",
  "//": "Make sure Ollama is running locally before using this configuration",
  "//": "Copy this to .openjragent.json in your project root or use as reference for ~/.openjragent/config.json",

  "agent": {
    "maxIterations": 8,
    "enableReflection": true,
    "requireConfirmation": false,
    "autoSave": true,
    "saveInterval": 60000
  },

  "llm": {
    "planner": {
      "provider": "ollama",
      "model": "codellama",
      "baseURL": "http://localhost:11434",
      "temperature": 0.7,
      "maxTokens": 4096,
      "timeout": 120000
    },
    "executor": {
      "provider": "ollama",
      "model": "codellama",
      "baseURL": "http://localhost:11434",
      "temperature": 0.3,
      "maxTokens": 4096,
      "timeout": 180000
    },
    "reflector": {
      "provider": "ollama",
      "model": "llama2",
      "baseURL": "http://localhost:11434",
      "temperature": 0.5,
      "maxTokens": 2048,
      "timeout": 120000
    }
  },

  "tools": {
    "enabled": [
      "code_query",
      "file_read",
      "file_write",
      "file_list",
      "snippet_save",
      "snippet_load",
      "snippet_list",
      "shell_exec",
      "ask_user"
    ],
    "workspaceDir": ".workspace",
    "maxFileSize": 10485760,
    "allowedExtensions": [
      ".js", ".ts", ".tsx", ".jsx",
      ".py", ".java", ".go", ".rs",
      ".json", ".yaml", ".yml",
      ".md", ".txt"
    ]
  },

  "logging": {
    "level": "info",
    "outputDir": "logs",
    "consoleOutput": true,
    "fileOutput": true,
    "format": "json",
    "maxFiles": 7,
    "maxSize": 10485760
  },

  "cli": {
    "theme": "dark",
    "showProgress": true,
    "confirmDangerous": true,
    "defaultTUI": true
  },

  "retry": {
    "maxRetries": 5,
    "initialDelay": 2000,
    "maxDelay": 15000,
    "strategy": "exponential"
  },

  "errorHandling": {
    "enableAutoRetry": true,
    "enableStateRollback": true,
    "enableLLMFallback": false,
    "maxSnapshotHistory": 10
  },

  "//note": "Ollama models may be slower and require higher timeouts",
  "//note2": "No API key is required for Ollama - just ensure the service is running",
  "//note3": "Popular models: codellama, llama2, mistral, deepseek-coder"
}
